This repo contains the dataset and related materials associated with the paper: 

*Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs Ability To Mark Short Answer Questions in K-12 Education*

This dataset is available for use for non-commercial uses only, with attribution (CC-BY-NC 4.0)

Final Paper : https://dl.acm.org/doi/10.1145/3657604.3664693

Pre-Print: https://arxiv.org/abs/2405.02985

To Cite

@inproceedings{10.1145/3657604.3664693,
author = {Henkel, Owen and Hills, Libby and Boxer, Adam and Roberts, Bill and Levonian, Zach},
title = {Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs Ability To Mark Short Answer Questions in K-12 Education},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3664693},
doi = {10.1145/3657604.3664693},
abstract = {This paper presents reports on a series of experiments with a novel dataset evaluating how well Large Language Models (LLMs) can mark (i.e. grade) open text responses to short answer questions, Specifically, we explore how well different combinations of GPT version and prompt engineering strategies performed at marking real student answers to short answer across different domain areas (Science and History) and grade-levels (spanning ages 5-16) using a new, never-used-before dataset from Carousel, a quizzing platform. We found that GPT-4, with basic few-shot prompting performed well (Kappa, 0.70) and, importantly, very close to human-level performance (0.75). This research builds on prior findings that GPT-4 could reliably score short answer reading comprehension questions at a performance-level very close to that of expert human raters. The proximity to human-level performance, across a variety of subjects and grade levels suggests that LLMs could be a valuable tool for supporting low-stakes formative assessment tasks in K-12 education and has important implications for real-world education delivery.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {300â€“304},
numpages = {5},
keywords = {formative assessment, llms, science education},
location = {Atlanta, GA, USA},
series = {L@S '24}
}
